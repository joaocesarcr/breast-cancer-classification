{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rfuvP_S3ydlV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from confusionmatrix import *\n",
        "from train import train\n",
        "from GAN import *\n",
        "from math import sqrt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "y9eUkic65__C"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def train(X_train, y_train, X_test, y_test):\n",
        "  scaler = StandardScaler()\n",
        "  X_test_scaled = scaler.fit_transform(X_test)\n",
        "  X_train_scaled = scaler.transform(X_train)\n",
        "\n",
        "  # KNN\n",
        "  knn_classifier = KNeighborsClassifier(n_neighbors=int(sqrt(len(X_train))))\n",
        "  knn_classifier.fit(X_train_scaled, y_train)\n",
        "  knn_predictions = knn_classifier.predict(X_test_scaled)\n",
        "  knn = confusionMatrix(y_test, knn_predictions)\n",
        "\n",
        "  # Logistic Regression\n",
        "  logreg_classifier = LogisticRegression()\n",
        "  logreg_classifier.fit(X_train_scaled, y_train)\n",
        "  logreg_predictions = logreg_classifier.predict(X_test_scaled)\n",
        "  logreg = confusionMatrix(y_test, logreg_predictions)\n",
        "\n",
        "  # SVM\n",
        "  svm_classifier = SVC(kernel='linear')\n",
        "  svm_classifier.fit(X_train_scaled, y_train)\n",
        "  svm_predictions = svm_classifier.predict(X_test_scaled)\n",
        "  svm = confusionMatrix(y_test, svm_predictions)\n",
        "\n",
        "  # Random Forest\n",
        "  rf_classifier = RandomForestClassifier(n_estimators=100)\n",
        "  rf_classifier.fit(X_train_scaled, y_train)\n",
        "  rf_predictions = rf_classifier.predict(X_test_scaled)\n",
        "  rf = confusionMatrix(y_test, rf_predictions)\n",
        "\n",
        "  return [knn, logreg, svm, rf]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SGIuS_tt6AuA"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Generator\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "          super(Generator, self).__init__()\n",
        "          layers = []\n",
        "          prev_size = input_size\n",
        "          for size in hidden_sizes:\n",
        "              layers.append(nn.Linear(prev_size, size))\n",
        "              layers.append(nn.LeakyReLU(0.2))\n",
        "              if not (size == hidden_sizes[-1]):\n",
        "                layers.append(nn.Dropout(0.5))\n",
        "              prev_size = size\n",
        "          layers.append(nn.Linear(prev_size, output_size))\n",
        "          layers.append(nn.Tanh())\n",
        "          self.layers = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Discriminator\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes):\n",
        "        super(Discriminator, self).__init__()\n",
        "        layers = []\n",
        "        prev_size = input_size\n",
        "        output_size = 1\n",
        "        for size in hidden_sizes:\n",
        "          layers.append(nn.Linear(prev_size, size))\n",
        "          layers.append(nn.LeakyReLU(0.2))\n",
        "          layers.append(nn.Dropout(0.5))\n",
        "          prev_size = size\n",
        "        layers.append(nn.Linear(prev_size, output_size))\n",
        "        layers.append(nn.Sigmoid())\n",
        "        self.layers = nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "      return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bgzIYzw-ehEo"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "scaler = StandardScaler()\n",
        "scaled = scaler.fit_transform(data.data)\n",
        "\n",
        "train_return = [\"knn\", \"logreg\", \"svm\", \"randomforest\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "KvDrFQY0QZw0"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "from random import randint\n",
        "\n",
        "import numpy as np\n",
        "def join_lists_of_lists(list1, list2):\n",
        "    if len(list1) != len(list2):\n",
        "        raise ValueError(\"Input lists must have the same number of sublists\")\n",
        "\n",
        "    result = []\n",
        "    for sublist1, sublist2 in zip(list1, list2):\n",
        "        result.append(sublist1 + sublist2)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def divide_list_into_k_parts(input_list, k):\n",
        "    n = len(input_list)\n",
        "    avg = n // k\n",
        "    remainder = n % k\n",
        "\n",
        "    result = []\n",
        "    start = 0\n",
        "\n",
        "    for i in range(k):\n",
        "        end = start + avg + (1 if i < remainder else 0)\n",
        "        result.append(input_list[start:end])\n",
        "        start = end\n",
        "    return result\n",
        "\n",
        "def kcv(data,target,k,print_info=False,randomize=False):\n",
        "\n",
        "    random_state = randint(0,100) if randomize else 42\n",
        "    data, target = shuffle(data,target, random_state=random_state)\n",
        "\n",
        "    b_rows = []\n",
        "    m_rows = []\n",
        "\n",
        "    for i, row in enumerate(data):\n",
        "        if target[i] == 1:\n",
        "            # Add row as an element of b_rows\n",
        "            b_rows += [row]\n",
        "        else:\n",
        "            m_rows += [row]\n",
        "\n",
        "\n",
        "    proportion = len(b_rows)/len(m_rows)\n",
        "    if (print_info):\n",
        "      print(f'Benign to Malignant proportion: {proportion} ({len(b_rows)}/{len(m_rows)})')\n",
        "\n",
        "    # Use a list comprehension to create new arrays with the added element\n",
        "    b_rows = [np.append(arr, 0) for arr in b_rows]\n",
        "    m_rows = [np.append(arr, 1) for arr in m_rows]\n",
        "\n",
        "    b_splits = divide_list_into_k_parts(b_rows, k)\n",
        "    m_splits = divide_list_into_k_parts(m_rows, k)\n",
        "\n",
        "    folds = np.array(join_lists_of_lists(b_splits, m_splits), dtype=object)\n",
        "\n",
        "    partitions = []\n",
        "\n",
        "    for i in range(k):\n",
        "        X_test = np.array([arr[:-1] for arr in folds[i]])\n",
        "        y_test = np.array([arr[-1] for arr in folds[i]])\n",
        "\n",
        "        X_train = np.concatenate([([arr[:-1] for arr in folds[j]]) for j in range(k) if j != i])\n",
        "        y_train = np.concatenate([([arr[-1] for arr in folds[j]]) for j in range(k) if j != i])\n",
        "\n",
        "        # Calculate the mean and standard deviation of each column\n",
        "        mean = np.mean(X_train, axis=0)\n",
        "        std_dev = np.std(X_train, axis=0)\n",
        "\n",
        "        # Subtract the mean from each element and divide by the standard deviation\n",
        "        training_set = (X_train - mean) / std_dev\n",
        "        test_set = (X_test - mean) / std_dev\n",
        "\n",
        "\n",
        "        #training_set, y_train = shuffle(training_set, y_train, random_state=random_state)\n",
        "        #test_set, y_test = shuffle(test_set, y_test, random_state=random_state)\n",
        "\n",
        "\n",
        "        if (print_info):\n",
        "          print(f'Test fold {i + 1}: Instances for training: {len(training_set)}, Instances for testing: {len(test_set)})')\n",
        "        partitions.append((training_set, y_train, X_test, y_test))\n",
        "\n",
        "\n",
        "    return partitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "l5koLAZbdESU"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "losses = []\n",
        "datasets = []\n",
        "\n",
        "def generate_data(dataset, hidden_size):\n",
        "    print(f\"Training GAN: {hidden_size}\")\n",
        "    \n",
        "    input_size = dataset.shape[1]  # Assuming input_size is defined somewhere\n",
        "    batch_size = 64  # Define your desired batch size\n",
        "    num_epochs = 2000  # Define your desired number of epochs\n",
        "    learning_rate = 0.0002  # Define your desired learning rate\n",
        "    num_samples = 1000  # Define the number of samples you want to generate\n",
        "    \n",
        "    # Create the generator and discriminator\n",
        "    generator = Generator(input_size, hidden_size, input_size).to(device)\n",
        "    discriminator = Discriminator(input_size, hidden_size).to(device)\n",
        "    \n",
        "    # Loss and optimizers\n",
        "    criterion = nn.BCELoss()\n",
        "    generator_optimizer = optim.Adam(generator.parameters(), lr=learning_rate)\n",
        "    discriminator_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
        "\n",
        "    results = []\n",
        "    g_loss_list = []\n",
        "    d_loss_list = []\n",
        "    for epoch in range(num_epochs):\n",
        "        # Train Discriminator\n",
        "        discriminator.zero_grad()\n",
        "        real_data = torch.tensor(dataset[np.random.choice(dataset.shape[0], batch_size)], dtype=torch.float32).to(device)\n",
        "        real_labels = torch.ones(batch_size, 1, dtype=torch.float32).to(device)\n",
        "\n",
        "        fake_data = generator(torch.randn(batch_size, input_size, dtype=torch.float32).to(device))\n",
        "        fake_labels = torch.zeros(batch_size, 1, dtype=torch.float32).to(device)\n",
        "\n",
        "        real_outputs = discriminator(real_data)\n",
        "        fake_outputs = discriminator(fake_data.detach())\n",
        "\n",
        "        d_loss_real = criterion(real_outputs, real_labels)\n",
        "        d_loss_fake = criterion(fake_outputs, fake_labels)\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        d_loss_list.append(d_loss.item())\n",
        "\n",
        "        d_loss.backward()\n",
        "        discriminator_optimizer.step()\n",
        "\n",
        "        # Train Generator\n",
        "        generator.zero_grad()\n",
        "        fake_data = generator(torch.randn(batch_size, input_size, dtype=torch.float32).to(device))\n",
        "        fake_outputs = discriminator(fake_data)\n",
        "        g_loss = criterion(fake_outputs, real_labels)\n",
        "        g_loss_list.append(g_loss.item())\n",
        "        g_loss.backward()\n",
        "        generator_optimizer.step()\n",
        "\n",
        "        # Print progress\n",
        "        if epoch % 500 == 0:\n",
        "            #print(f\"    Epoch [{epoch}/{num_epochs}] D Loss: {d_loss.item():.4f} G Loss: {g_loss.item():.4f}\")\n",
        "            pass\n",
        "\n",
        "    # Generate synthetic data\n",
        "    synthetic_data = generator(torch.randn(num_samples, input_size, dtype=torch.float32).to(device)).detach().cpu().numpy()\n",
        "\n",
        "    return synthetic_data\n",
        "\n",
        "# Call generate_data with your dataset and hidden_size\n",
        "# generated_data = generate_data(your_dataset, your_hidden_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Wj1CYV9EjKkq"
      },
      "outputs": [],
      "source": [
        "def graph_losses(losses_list):\n",
        "  l = np.array(losses[datasets.index(max(datasets))])\n",
        "  plt.plot(l[0],label=\"Generator\")\n",
        "  plt.plot(l[1],label=\"Discriminator\")\n",
        "  plt.legend(loc='upper right')\n",
        "\n",
        "  l = np.array(losses[datasets.index(min(datasets))])\n",
        "  plt.plot(l[0],label=\"Generator\")\n",
        "  plt.plot(l[1],label=\"Discriminator\")\n",
        "  plt.legend(loc='upper right')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "A3w28_Tx2-Ze"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "def get_true_false_list(y_train):\n",
        "  \"\"\"\n",
        "  Given the list of targets, return the indexes of positives and negatives\n",
        "  \"\"\"\n",
        "  y_true = []\n",
        "  y_false = []\n",
        "  for i in range(len(y_train)):\n",
        "    if (y_train[i] == 0):\n",
        "      y_false.append(i)\n",
        "    else:\n",
        "      y_true.append(i)\n",
        "  return y_true, y_false\n",
        "\n",
        "\n",
        "def create_table(X_train, y_true_indexes, y_false_indexes,hidden_size):\n",
        "  # Given a table, return a synthetic generated table\n",
        "  gan_results = []\n",
        "  true_data = generate_data(X_train[y_true_indexes],hidden_size) # Train a GAN on the positive data\n",
        "  true_labels = np.ones((true_data.shape[0],1))\n",
        "  true_table = np.concatenate((true_data,true_labels),axis=1)\n",
        "\n",
        "  false_data = generate_data(X_train[y_false_indexes],hidden_size) # Train a GAN on the false data\n",
        "  false_labels = np.zeros((false_data.shape[0],1))\n",
        "  false_table = np.concatenate((false_data,false_labels),axis=1)\n",
        "\n",
        "  true_rows =  true_table[np.random.choice(len(true_table), len(y_true_indexes))]\n",
        "  false_rows =  false_table[np.random.choice(len(false_table),len(y_false_indexes))]\n",
        "\n",
        "  final_table = np.concatenate((true_rows,false_rows),axis=0)\n",
        "  final_table = shuffle(final_table)\n",
        "\n",
        "  return final_table\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zDu0zloXf7B-"
      },
      "outputs": [],
      "source": [
        "# GAN parameters\n",
        "GANS = [\n",
        "    [256,512],\n",
        "    [256],\n",
        "    [128,256],\n",
        "    [128],\n",
        "]\n",
        "# Hyperparameters\n",
        "input_size = 30 # n atributos\n",
        "batch_size = 16\n",
        "learning_rate = 0.0012\n",
        "num_epochs = 5000\n",
        "num_samples = 569 # mesma qtt de dados no dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Treina as GANs e depois os algoritmos usando os dados reais e sintéticos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tLPt5DLvq4TA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "Training GAN: [256, 512]\n",
            "Training GAN: [256, 512]\n",
            "Training GAN: [256]\n",
            "Training GAN: [256]\n",
            "Training GAN: [128, 256]\n",
            "Training GAN: [128, 256]\n",
            "Training GAN: [128]\n",
            "Training GAN: [128]\n",
            "1\n",
            "Training GAN: [256, 512]\n",
            "Training GAN: [256, 512]\n",
            "Training GAN: [256]\n",
            "Training GAN: [256]\n",
            "Training GAN: [128, 256]\n",
            "Training GAN: [128, 256]\n",
            "Training GAN: [128]\n",
            "Training GAN: [128]\n",
            "2\n",
            "Training GAN: [256, 512]\n",
            "Training GAN: [256, 512]\n",
            "Training GAN: [256]\n",
            "Training GAN: [256]\n",
            "Training GAN: [128, 256]\n",
            "Training GAN: [128, 256]\n",
            "Training GAN: [128]\n",
            "Training GAN: [128]\n",
            "3\n",
            "Training GAN: [256, 512]\n",
            "Training GAN: [256, 512]\n",
            "Training GAN: [256]\n",
            "Training GAN: [256]\n",
            "Training GAN: [128, 256]\n",
            "Training GAN: [128, 256]\n",
            "Training GAN: [128]\n",
            "Training GAN: [128]\n",
            "4\n",
            "Training GAN: [256, 512]\n",
            "Training GAN: [256, 512]\n",
            "Training GAN: [256]\n",
            "Training GAN: [256]\n",
            "Training GAN: [128, 256]\n",
            "Training GAN: [128, 256]\n",
            "Training GAN: [128]\n",
            "Training GAN: [128]\n"
          ]
        }
      ],
      "source": [
        "repetitions_per_gan = 1\n",
        "k_number = 5;\n",
        "folds_results_natural=[]\n",
        "folds_results_gan_all=[]\n",
        "def results():\n",
        "  folds = kcv(data.data, data.target,k_number)\n",
        "  for i,fold in enumerate(folds):\n",
        "    print(i)\n",
        "    X_train, y_train, X_test, y_test = fold\n",
        "    results = train(X_train, y_train, X_test, y_test)\n",
        "\n",
        "    y_true_indexes, y_false_indexes = get_true_false_list(y_train)\n",
        "    for hidden_size in GANS: # For each GAN sizes\n",
        "      final_table = create_table(X_train,y_true_indexes,y_false_indexes,hidden_size)\n",
        "      train_results = train(final_table[:,:30], final_table[:,30], X_test, y_test)\n",
        "      folds_results_gan_all.append((train_results))\n",
        "\n",
        "    folds_results_natural.append((results))\n",
        "\n",
        "results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MrlzGjiFwS6X"
      },
      "outputs": [],
      "source": [
        "# Comparar os dados sinteticos e naturais\n",
        "# Mais arquiteturas\n",
        "def get_attr_from_list(train_return, info):\n",
        "  knn, logreg, svm, rf = train_return\n",
        "  return [getattr(model,info) for model in train_return]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imprime uma tabela para os dados originais, imprimindo avg(stdev) de todos os atributos para cada algorítmo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yEOM7hxD2-ss"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "natural results\n",
            "+-----------+--------------+--------------+--------------+--------------+\n",
            "| Attribute |     knn      |    logreg    |     svm      | randomforest |\n",
            "+-----------+--------------+--------------+--------------+--------------+\n",
            "|  accuracy | 0.961(0.012) | 0.981(0.012) | 0.979(0.013) | 0.956(0.009) |\n",
            "| precision | 0.995(0.011) | 0.991(0.013) | 0.986(0.013) | 0.948(0.025) |\n",
            "|   recall  | 0.901(0.032) | 0.957(0.035) | 0.957(0.031) | 0.934(0.010) |\n",
            "|  f_score  | 0.945(0.018) | 0.973(0.016) | 0.971(0.019) | 0.941(0.012) |\n",
            "|    npv    | 0.944(0.016) | 0.976(0.020) | 0.975(0.018) | 0.961(0.006) |\n",
            "+-----------+--------------+--------------+--------------+--------------+\n"
          ]
        }
      ],
      "source": [
        "from prettytable import PrettyTable\n",
        "import statistics\n",
        "\n",
        "t = PrettyTable(['Attribute','knn', 'logreg','svm','randomforest'])\n",
        "print(\"natural results\")\n",
        "cm_attrs = [\"accuracy\",\"precision\",\"recall\",\"f_score\",\"npv\"]\n",
        "for attr in cm_attrs:\n",
        "  table = [0,0,0,0]\n",
        "  table2 = [0,0,0,0]\n",
        "\n",
        "  knnres=[]\n",
        "  logregres=[]\n",
        "  svmres=[]\n",
        "  rfres=[]\n",
        "  for natural_result in folds_results_natural:\n",
        "    \n",
        "    atributes =  (get_attr_from_list(natural_result,attr))\n",
        "    knnres.append(atributes[0])\n",
        "    logregres.append(atributes[1])\n",
        "    svmres.append(atributes[2])\n",
        "    rfres.append(atributes[3])\n",
        "    table = [x + y for x, y in zip(table, atributes)]\n",
        "  table2 =[attr]\n",
        "  table2.extend([statistics.stdev(knnres),statistics.stdev(logregres),statistics.stdev(svmres),statistics.stdev(rfres)])\n",
        "  table = [x / k_number for x in table]\n",
        "  table.insert(0,attr)\n",
        "  tableText=[attr];\n",
        "  for i in range(1,len(table)):\n",
        "    tableText.append(\"{:.3f}({:.3f})\".format(table[i],table2[i]))\n",
        "  t.add_row(tableText)\n",
        "\n",
        "t.float_format = '.3'\n",
        "print(t)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imprime uma tabela para cada arquitetura da GAN, imprimindo avg(stdev) de todos os atributos para cada algorítmo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gan results\n",
            "+-----------+--------------+--------------+--------------+--------------+\n",
            "| Attribute |     knn      |    logreg    |     svm      | randomforest |\n",
            "+-----------+--------------+--------------+--------------+--------------+\n",
            "|  accuracy | 0.909(0.022) | 0.917(0.027) | 0.914(0.041) | 0.880(0.050) |\n",
            "| precision | 0.989(0.026) | 0.988(0.026) | 0.982(0.041) | 0.974(0.024) |\n",
            "|   recall  | 0.764(0.059) | 0.787(0.064) | 0.783(0.095) | 0.697(0.138) |\n",
            "|  f_score  | 0.861(0.036) | 0.875(0.044) | 0.869(0.067) | 0.807(0.093) |\n",
            "|    npv    | 0.877(0.028) | 0.888(0.030) | 0.886(0.045) | 0.850(0.060) |\n",
            "+-----------+--------------+--------------+--------------+--------------+\n",
            "+-----------+--------------+--------------+--------------+--------------+\n",
            "| Attribute |     knn      |    logreg    |     svm      | randomforest |\n",
            "+-----------+--------------+--------------+--------------+--------------+\n",
            "|  accuracy | 0.935(0.028) | 0.953(0.013) | 0.951(0.015) | 0.902(0.033) |\n",
            "| precision | 0.994(0.012) | 1.000(0.000) | 0.990(0.014) | 0.995(0.012) |\n",
            "|   recall  | 0.830(0.073) | 0.873(0.036) | 0.877(0.051) | 0.741(0.095) |\n",
            "|  f_score  | 0.904(0.043) | 0.932(0.020) | 0.929(0.023) | 0.846(0.060) |\n",
            "|    npv    | 0.909(0.037) | 0.930(0.019) | 0.933(0.027) | 0.868(0.042) |\n",
            "+-----------+--------------+--------------+--------------+--------------+\n",
            "+-----------+--------------+--------------+--------------+--------------+\n",
            "| Attribute |     knn      |    logreg    |     svm      | randomforest |\n",
            "+-----------+--------------+--------------+--------------+--------------+\n",
            "|  accuracy | 0.910(0.038) | 0.909(0.039) | 0.917(0.042) | 0.872(0.051) |\n",
            "| precision | 0.981(0.027) | 0.987(0.019) | 0.967(0.027) | 0.967(0.056) |\n",
            "|   recall  | 0.773(0.091) | 0.764(0.094) | 0.806(0.115) | 0.679(0.120) |\n",
            "|  f_score  | 0.863(0.064) | 0.859(0.068) | 0.875(0.073) | 0.793(0.091) |\n",
            "|    npv    | 0.882(0.042) | 0.878(0.043) | 0.898(0.053) | 0.840(0.052) |\n",
            "+-----------+--------------+--------------+--------------+--------------+\n",
            "+-----------+--------------+--------------+--------------+--------------+\n",
            "| Attribute |     knn      |    logreg    |     svm      | randomforest |\n",
            "+-----------+--------------+--------------+--------------+--------------+\n",
            "|  accuracy | 0.933(0.024) | 0.945(0.027) | 0.953(0.019) | 0.914(0.045) |\n",
            "| precision | 1.000(0.000) | 0.984(0.025) | 0.984(0.024) | 1.000(0.000) |\n",
            "|   recall  | 0.821(0.064) | 0.868(0.058) | 0.887(0.036) | 0.769(0.120) |\n",
            "|  f_score  | 0.901(0.037) | 0.921(0.041) | 0.933(0.028) | 0.865(0.080) |\n",
            "|    npv    | 0.905(0.032) | 0.927(0.030) | 0.937(0.019) | 0.882(0.054) |\n",
            "+-----------+--------------+--------------+--------------+--------------+\n"
          ]
        }
      ],
      "source": [
        "from prettytable import PrettyTable\n",
        "print(\"gan results\")\n",
        "cm_attrs = [\"accuracy\",\"precision\",\"recall\",\"f_score\",\"npv\"]\n",
        "architecture256_512 = folds_results_gan_all[0::4]\n",
        "architecture256 = folds_results_gan_all[1::4]\n",
        "architecture128_256 = folds_results_gan_all[2::4]\n",
        "architecture128 = folds_results_gan_all[3::4]\n",
        "archAll=[architecture256_512,architecture256,architecture128_256,architecture128]\n",
        "for arch in archAll:\n",
        "  t = PrettyTable(['Attribute','knn', 'logreg','svm','randomforest'])\n",
        "\n",
        "  for attr in cm_attrs:\n",
        "    table = [0,0,0,0];\n",
        "    table2 = [0,0,0,0]\n",
        "    knnres=[]\n",
        "    logregres=[]\n",
        "    svmres=[]\n",
        "    rfres=[]\n",
        "\n",
        "    for gan_result in arch:\n",
        "      atributes =  (get_attr_from_list(gan_result,attr))\n",
        "      knnres.append(atributes[0])\n",
        "      logregres.append(atributes[1])\n",
        "      svmres.append(atributes[2])\n",
        "      rfres.append(atributes[3])\n",
        "      table = [x + y for x, y in zip(table, atributes)]\n",
        "\n",
        "    table2 =[attr]\n",
        "    table2.extend([statistics.stdev(knnres),statistics.stdev(logregres),statistics.stdev(svmres),statistics.stdev(rfres)])\n",
        "    table = [x / k_number for x in table]\n",
        "    \n",
        "    table.insert(0,attr)\n",
        "    tableText=[attr];\n",
        "    for i in range(1,len(table)):\n",
        "      tableText.append(\"{:.3f}({:.3f})\".format(table[i],table2[i]))\n",
        "    t.add_row(tableText)\n",
        "\n",
        "  t.float_format = '.3'\n",
        "  print(t)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
